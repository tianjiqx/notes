# 存算分离


## 趋势

- 学术界：新硬件（CXL）关注内存解耦，原生分离架构的系统
- 工业界：现有系统的存算分离实现，典型应用场景（OLAP） 云原生架构


### 网络改善


硬件：


- Basic/Foundational NIC
    - Usually 1Gbps – 25Gbps
    - Relies on CPU for protocol processing – ≥ 30% server

- Smart NIC
    - ≥ 50Gbps
    - Offload network protocol processing
    - Have their own processor, memory & OS

- DPU
    - Smart NIC + security + storage
    - Custom chips and/or FPGAs  

T. Döring et al., SmartNICs: Current Trends in Research and Industry, 2020
https://www.net.in.tum.de/fileadmin/TUM/NET/NET-2021-05-1/NET-2021-05-1_05.pdf

软件：

- High overhead of TCP
    - 100+ GB
- Low-overhead protocol
    - RDMA (Infiniband, RoCE)
    - CXL(Compute Express Link)
- Efficient RPC protocol – higher level
    - ack ？ udp协议？


RDMA
- Copies data from application memory to NIC
- Copies data across the network
- Requires handling of cache coherence (in some configurations)
- Requires a CPU/controller at memory nodes

CXL
- Does not copy data into NIC buffers
- Does not copy data across the network, accesses remote memory
- Provides a hardware supported coherent cache
- No need for CPU/controller at memory nodes


8x RDMA


1 L. Barroso, et al. Attack of the Killer Microseconds. Commun. ACM, 2017.
2 J. Ousterhout. It’s Time to Replace TCP in the Datacenter, arXiv 2210.00714, 2023.
D. Gouk, et al. Direct Access, High Performance Memory Disaggregation with DirectCXL. Proc. USENIX ATC, 2022.



#### REF

- Disaggregated & Heterogeneous Platform for Data Management . cs848-w2024

## 问题

### shared-nothing架构下的LSM-based 数据库

- 不同分片数据不均衡，某些热点数据处理压力大
- 后台进行flush/compaction或者数据迁移时产生的抖动
- 内存是昂贵的开销
    - DRAM is an expensive resource in the cloud – 50% of server cost on Azure
    - Memory utilization is low in the (current) cloud


### 存算分离优势


- 性能提升
    - 独立,弹性扩展：存算分离允许计算资源和存储资源独立扩展，这意味着可以根据业务需求单独增加计算能力或存储容量，从而优化整体性能。
    - 资源利用率高：计算和存储任务被分配给专门设计的资源池，计算池专注于处理查询，存储池专注于高效数据存储（内存池）
- 可靠性增强    
    - 数据高可用：在存算分离架构中，数据存储在高可靠的专业存储系统中，即使计算节点故障，数据依然安全，确保业务连续性。
    - 故障容忍：高端存储系统设计有高故障容忍能力，减少硬件冗余需求，提升整体可靠性。
    - 计算节点快速恢复、扩展：无需恢复数据
- 降低成本：通过灵活扩展计算和存储资源，可以降低系统的整体成本

### 存算分离难点

- 网络延迟：计算节点和存储节点之间的通信需要通过网络进行，网络状况直接影响写入、查询响应速度
    - 缓存、压缩
    - 近数计算  near-data computing， log-as-the-database

- 资源共享
    - 分布式共享存储、共享内存的可以被计算节点使用（内存的一致性、并发控制）
    - 弹性扩展


## 实现

### OLTP

- Amazon Aurora
- Microsoft Socrates
- Google AlloyDB
- Alibaba PolarDB
    - PolarFS

### OLAP

- Snowflake
    - 存储层
        - 基于S3的高可用和持久行
            - 慢，但是廉价和可用
            - 计算节点依赖缓存
        - Partition table into file
            - 文件大小 16MB
        - PAX hybrid columnar storage format
        - 所有计算节点可以访问所有存储数据
    - 计算层
        - Virtual Warehouse (VW)，MPP
        - Elasticity
    - 控制层
        - 元信息，计划，访问控制
- Amazon Redshift

- databend
- starrock
- tiflash

### 组件

广泛的架构组件设计

- Compute Node 计算节点（leader,workers）
- cache 数据缓存
- metadata 元信息
    - 文件管理
- Storage Node 存储节点（可选）
    - 数据过滤
    - 共享存储数据管理:compact 
    - eg. TiFlash Write Node
- 共享存储（OSS,HDFS）、文件系统
    - 冷数据 parquet 文件存储格式 

### 思考

- 存储引擎的必要性与位置
    - 数据过滤计算，访问文件，写入处理
    - 单独服务 （tiflash） or 计算存储一体
- 对象存储中的数据内容
    - 索引，元数据
    - block 大小，segment文件块 or 列文件块
    - 原始segment or parquet


## REF

### 学术
- [cs848-w2024](https://ozsu.github.io/cs848-w2024/Schedule.html) slides 推荐

- [SIGMOD23_DisaggregatedDB_Slides](https://www.cs.purdue.edu/homes/csjgwang/pubs/SIGMOD23_DisaggregatedDB_Slides.pdf)
    - [papaer](https://www.cs.purdue.edu/homes/csjgwang/pubs/SIGMOD23_Tutorial_DisaggregatedDB.pdf)

- [ASPLOS2020 Hailstorm: Disaggregated Compute and Storage for Distributed LSM-based Databases 笔记](https://zhuanlan.zhihu.com/p/381280016)
    - 计算/存储资源的解耦合
    - 存储池化（small block（1MB））
        - 在各shard下的storage engine引入一层shared-storage的存储层进行池化，分摊了io开销和消除io热点，消除数据倾斜和单机disk的瓶颈
    - Compaction/flush任务的均衡调度
        - 提供task调度功能，将compaction/flush任务根据io/cpu负载在不同分片中进行调度
        - HailStorm Agent类似于一个compaction/flush任务的scheduler，把io/cpu intensive的task分发到利用率比较低的节点上
        - 计算层的compaction offloading用于减缓cpu load
    - 基于文件系统操作（FSClient）
        - storage engine中对compaction/flushing是对文件进行操作的
        - 文件系统提供如mmap 内存映射方法
    - 元数据存储
        - 每个文件通过全局唯一的uuid来识别
        - 每个文件的block顺序存储，方便定位
        - 每个hailstorm client存储着文件路径和uuid的映射（全局元信息）
    - 读优化
        - 读取比写入时，block的粒度会更小，来减少传输的latency和读放大
        - compaction/flush操作则使用默认的block粒度来保证写入性能
    - 容错
        - LSM用WAL实现崩溃恢复保证数据一致性
        - 分布式数据库通过跨机器/跨机架/跨AZ/跨数据中心的备份来保持高可用
        - rack： RAID保证冗余
        - 文件的metadata存储在本地（bad, pg,mysql 主备，etcd等）
    
### 实践

- [ClickHouse 存算分离架构探索](https://zhuanlan.zhihu.com/p/357451583) 

- [全新存算分离架构——[SIGMOD2021] PolarDB Serverless: A Cloud Native Database for Disaggregated Data Centers 笔记](https://zhuanlan.zhihu.com/p/382109937)

- [存算分离/DB on K8s 论文/blog收集](https://zhuanlan.zhihu.com/p/377755864)

- [存算分离下写性能提升10倍以上，EMR Spark引擎是如何做到的？](https://zhuanlan.zhihu.com/p/272202352) 

- [TiFlash 存算分离架构与 S3 支持](https://docs.pingcap.com/zh/tidb/stable/tiflash-disaggregated-and-s3#tiflash-%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB%E6%9E%B6%E6%9E%84%E4%B8%8E-s3-%E6%94%AF%E6%8C%81)
    - 存算分离架构中，TiFlash 原有进程的不同部分的功能，被拆分到两种不同的节点
        - TiFlash Write Node
            - 负责接收 TiKV 的 Raft logs 数据，将数据转换成列存格式，并每隔一小段时间将这段时间的所有数据更新打包上传到 S3 中
            - 管理 S3 上的数据，比如不断整理数据使之具有更好的查询性能，以及删除无用的数据等
        - TiFlash Compute Node 
            - 负责执行从 TiDB 节点发过来的查询请求
                - 首先访问 Write Node 以获取数据的快照 (data snapshots)
                - 然后分别从 Write Node 读取最新的数据（即尚未上传到 S3 的数据）
                - 从 S3 读取剩下的大部分数据
            - Compute Node 利用本地磁盘（通常是 NVMe SSD）来作为数据文件的缓存，从而避免相同的数据反复从远端（Write Node 或者 S3）读取，以提高查询性能
![tiflash](./images/tiflash-s3.png)


- [StarRocks 存算分离 Data Cache 二三事](https://zhuanlan.zhihu.com/p/695673099)
    - Cache V1: File Cache,  LRU 淘汰, 同时写入对象存储和本地 Cache
        - 缺点：
            - 空间效率低， 可能访问只有部分列数
            - Cache Miss 时代价大
            - 淘汰效率低， 在本地文件系统上 list 上百万甚至千万的文件
            - Cache Disk 不均衡，将某个分区映射到特定的磁盘，数据分区大小不均衡导致磁盘使用不均
    - Cache V2: Block Cache（StarCache ）
        - 以特定大小 Block （典型大小如 1MB）为缓存单位的新型 Cache 系统
        - 80% 磁盘空间阈值
- [兼顾降本与增效，我们对存算分离的设计与思考](https://zhuanlan.zhihu.com/p/630277812)
    - 构建了一个统一的存算分离平台——StarOS， StarOS 的调度中枢负责计算任务调度
        - 基于 StarOS, 构建 StarRocks
        - FE
            - Compaction：将用户的历史版本合并为一个更大的版本以提高查询性能
                - 得益于存算分离的数据共享能力，FE 可以选择将 Compaction 任务发往任意 BE 节点执行
                - TODO 专用集群，避免干扰集群
        - BE/cache
    - 存储
        - 后端存储方式
            - 兼容 AWS S3 协议的对象存储系统
            - 传统数据中心部署的 HDFS
        - StarRocks 存算分离数据文件格式与存算一体相同，数据按照 Segment 文件组织，StarRocks 各种索引技术在存算分离表中也同样复用
        - 数据多版本
            - 每个数据版本包含 Tablet Meta 和 Tablet Data 文件，并且都写入后端对象存储
                - TabletMeta 文件内记录了该版本所有的数据文件索引
                - Tablet Data 文件仍然按照 Segment 文件格式组织
            - BE 节点需要访问某个Tablet 时，会先根据版本号从后端存储加载对应的 Tablet Meta 文件，然后再根据索引访问相应的数据文件
    - 缓存
        - 使用磁盘（Local Disk） 来缓存热点数据
    - 对比
        - 写入：随着并发提升，存算分离表的写入吞吐仍在不断提升，直到最终达到了 BE 节点的网络带宽瓶颈
        - 查询
            - 存算分离版本在缓存完全命中情况下，其总体性能已经追平存算一体（428s VS 423s）1.01X
            - 完全访问冷数据情况下，通过预读等优化手段，也很好地将冷数据查询性能衰退控制在一个合理范围内（668s VS 428s）  1.57X


- [万字长文|从AIGC典型客户实践揭秘云原生向量数据库内核设计与智能创新](https://mp.weixin.qq.com/s/KPN-JoICYnTf-wyB-Rmoig) 向量数据库的存算分离
    - 迭代过程：
        - 在PostgreSQL上实现了类似pgvector的向量索引插件，支持了高维向量的高效检索，支持了向量数据的实时更新等基础功能
            - 基于段页式存储的HNSW索引，3次遍历图索引
            - 问题：段页式存储带来的加锁访问开销占据了整个执行时间的1/3，大量随机图查询造成大量Shared Buffer页面申请淘汰
        - 基于Huge-Block的自研向量索引
            - 把向量索引的数据按照1GB大小为一块来申请
        - Relyt-V架构
            - DWSU 数仓服务单元 包含多种DPS数据处理服务集群，DPS共享一份数据，一个DPS为读写集群，其它DPS为只读集群
                - Hybrid DPS提供了数据实时写入，实时分析的能力，Extreme DPS提供了极速Ad Hoc查询，交互式分析能力，Spark DPS提供了离线分析，以及Vector DPS提供向量和全文的检索能力
            - Vector DPS
                - 典型的数据库架构，包含各种SQL计算和存储的实现， 支持的索引，包含B-tree、全文、JSON和向量索引
                - 计算层是PostgreSQL集群，负责向量的写入和查询
                - 中间层的Block Service提供PostgreSQL的Page回放和读服务，Log Service提供WAL日志的持久化服务，Index Service提供向量索引的构建服务
                    - 索引的服务化能力，提供索引的异步构建能力，同时索引构建不会影响上层计算的读写请求
                - 底层是对象存储，提供数据的持久化能力。
                - 存算分离改造
                    - 从它的WAL日志做了Hook，把WAL日志路由到Log Server，并通过Paxos协议保证WAL日志的高可靠
                    - PostgreSQL读写Page做了Hook，读Page路由到Block Server，Block Server从Log Server拉取WAL日志会回放成Page
                    - Log Server和Block Server定期会把自己的数据同步到对象存储持久化
                    - 持久化完成后，Log Server和Block Server就可以安全的清理自己的WAL日志和本地文件
                ![](./images/aigc1.webp)
                - 弹性Serverless
                    - Block Server的迁移（负载），PG重试读取Page
                    - Log Server 通过多副本的增减实现副本跨节点的迁移，有状态的网络中断、进程重启，通过QEMU实现的虚拟机实现进程的迁移，通过VXLAN的网络虚拟化能力，解决迁移过程中网络不中断的问题
                - 向量索引服务化
                    - LSM的向量索引存储引擎
                    - 按需拉起Index Build Service，通过从对象存储同步Vector数据来实现向量索引的构建

                ![](./images/aigc2.webp)


