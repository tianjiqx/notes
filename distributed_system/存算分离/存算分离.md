# 存算分离

## 趋势

- 学术界：新硬件（CXL）关注内存解耦，原生分离架构的系统
- 工业界：现有系统的存算分离实现，典型应用场景（OLAP） 云原生架构

### 网络改善

硬件：

- Basic/Foundational NIC
  
  - Usually 1Gbps – 25Gbps
  - Relies on CPU for protocol processing – ≥ 30% server

- Smart NIC
  
  - ≥ 50Gbps
  - Offload network protocol processing
  - Have their own processor, memory & OS

- DPU
  
  - Smart NIC + security + storage
  - Custom chips and/or FPGAs  

T. Döring et al., SmartNICs: Current Trends in Research and Industry, 2020
https://www.net.in.tum.de/fileadmin/TUM/NET/NET-2021-05-1/NET-2021-05-1_05.pdf

软件：

- High overhead of TCP
  - 100+ GB
- Low-overhead protocol
  - RDMA (Infiniband, RoCE)
  - CXL(Compute Express Link)
- Efficient RPC protocol – higher level
  - ack ？ udp协议？

RDMA

- Copies data from application memory to NIC
- Copies data across the network
- Requires handling of cache coherence (in some configurations)
- Requires a CPU/controller at memory nodes

CXL

- Does not copy data into NIC buffers
- Does not copy data across the network, accesses remote memory
- Provides a hardware supported coherent cache
- No need for CPU/controller at memory nodes

8x RDMA

1 L. Barroso, et al. Attack of the Killer Microseconds. Commun. ACM, 2017.
2 J. Ousterhout. It’s Time to Replace TCP in the Datacenter, arXiv 2210.00714, 2023.
D. Gouk, et al. Direct Access, High Performance Memory Disaggregation with DirectCXL. Proc. USENIX ATC, 2022.

#### REF

- Disaggregated & Heterogeneous Platform for Data Management . cs848-w2024

## 问题

### shared-nothing架构下的LSM-based 数据库

- 不同分片数据不均衡，某些热点数据处理压力大
- 后台进行flush/compaction或者数据迁移时产生的抖动
- 内存是昂贵的开销
  - DRAM is an expensive resource in the cloud – 50% of server cost on Azure
  - Memory utilization is low in the (current) cloud

### 存算分离优势

- 性能提升
  - 独立,弹性扩展：存算分离允许计算资源和存储资源独立扩展，这意味着可以根据业务需求单独增加计算能力或存储容量，从而优化整体性能。
  - 读写分离：负载互相不影响，资源隔离。（tiflash）
  - 资源利用率高：计算和存储任务被分配给专门设计的资源池，计算池专注于处理查询，存储池专注于高效数据存储（内存池）
- 可靠性增强    
  - 数据高可用：在存算分离架构中，数据存储在高可靠的专业存储系统中，即使计算节点故障，数据依然安全，确保业务连续性。
  - 故障容忍：高端存储系统设计有高故障容忍能力，减少硬件冗余需求，提升整体可靠性。
  - 计算节点快速恢复、扩展：无需恢复数据
- 降低成本：通过灵活扩展计算和存储资源，可以降低系统的整体成本

### 存算分离难点

- 网络延迟：计算节点和存储节点之间的通信需要通过网络进行，网络状况直接影响写入、查询响应速度
  
  - 缓存、压缩
  - 近数计算  near-data computing， log-as-the-database

- 资源共享
  
  - 分布式共享存储、共享内存的可以被计算节点使用（内存的一致性、并发控制）
  - 弹性扩展

- 数据近实时、一致性（not all system need）

## 实现

### OLTP

- Amazon Aurora

  - 数据持久化是以日志的形式去写的，需要通过跨 AZ 的数据同步实现快速的数据更新，基于底层的日志即数据、主从复制、多副本复制、共享存储，实现了快速的数据共享和弹性能力
  - 不足是底层的日志和存储都需要与这种架构相绑定，是高度定制化的，架构难以复制


- Microsoft Socrates
- Google AlloyDB
- Alibaba PolarDB
  - PolarFS


#### Log-as-the-Database

![](./images/Screenshot%202024-07-23%20at%204.29.02%E2%80%AFPM.png)

- 在事务提交时只向存储端发送预写日志
- 实际的数据页是通过在存储节点上异步重放日志生成的


查询
- 计算节点将首先检查本地缓冲区。如果存在缓存未命中，它将从存储节点获取未命中的页面。
- 如果请求的页面尚未重放，则存储节点将动态重放所需的日志（图4中的步骤1），并在完成时返回请求的页面（图4中的步骤2）。


与Remote Disk架构相比，LogDB只发送日志，因此能提高写性能。

![](./images/Screenshot%202024-07-23%20at%204.29.15%E2%80%AFPM.png)


##### Log-as-the-Database with Multi-Version Storage 

有一个主(计算)节点和多个辅助(计算)节点。只有主计算节点支持读写事务，而所有辅助节点只能处理只读事务。

支持共享存储设计引入多版本页面, eg. ali ADB ，支持横向无状态的扩展计算节点。

主要的问题，由于网络开销，会出现复制延迟问题。

GetPage@LSN：存储引擎必须支持多版本页面，因为辅助计算节点需要访问页面的旧版本



写入和回放xlog：

- 在存储节点接收到xlog之后，它会异步检查这个xlog中包含了哪些页面修改
- 存储节点将这个xlog的LSN和页面id的映射记录附加到“Version Map”，该“Version Map”是一个散列表，使用**页面id作为它的key**，**版本LSN列表**作为它的value（如图5中的步骤a所示），被GetPage@LSN使用
- 存储节点重放过程将xlog划分为几个mini-xlog，每个mini-xlog仅包含一个页面的修改
- 重放多个mini-xlog以获得多个更新的页面（图5中的步骤b）
- 存储节点将新生成的页面插入RocksDB中，使用页面id和LSN作为键，页面内容作为值（图5中的步骤c）


对于GetPage@LSN请求：
- 首先，存储节点等待replay process，直到它重放的xlog超过了请求参数中的LSN(图5中的步骤1)
- 然后，它使用请求参数中的page id，从Version Map中获取版本LSN列表。在列表中选择目标版本时，选择的就是小于等于参数中的LSN的那个最高的LSN(图5中的步骤2)
- 最后，存储节点将page id和版本LSN结合起来作为其键，从RocksDB中检索目标页面(图5中的步骤3)。

shared-storage设计原则的性能影响仍然是未知和复杂的
- bad： 支持多个版本会带来额外的性能开销，因为更高的xlog重放的开销(因为每个xlog现在可以表示多个页面)以及在向RocksDB插入页面时更高的开销(因为多版本导致页面总数大幅增加)
- good：多版本可以解决PG“写页撕裂”问题，这反过来又会提高I/O性能

[Deep dive into Neon storage engine :GetPage@LSN](https://neon.tech/blog/get-page-at-lsn)


键值存储

- key:  relation-id and block number 关系ID和块编号
- value: 一个8 KB页或一个预写日志（WAL）记录

类LSM
- 文件被创建和删除，不可变
- 传入的WAL流首先在内存中进行缓冲和组织。当累积了1GB的WAL时，它将被写入一个新的层文件。
- 通过创建新文件和删除旧文件来合并和压缩旧文件。

Neon 支持历史数据访问

写入路径
- 每当进行修改时，PostgreSQL都会将一条记录写入事务日志（预写日志）
- 日志被流传输到三个安全守护者节点safekeeper，这三个节点使用类似Paxos的一致性算法提供持久性和同步复制
- 安全守护者节点将日志流传输到页面服务器，在页面服务器中，日志被处理、重新组织并写入不可变文件。
- 最后，这些文件被上传到云存储进行长期存储。

读取路径
- 从计算节点到pageserver的每个读请求都包括一个日志序列号（LSN）
- 当一个读请求进来时，pageserver通过LSN找到页面的最后一个image，以及它上面的任何预写日志（WAL）记录，如果需要重新构建页面，则应用WAL记录，并将具体化的页面返回给调用者。将此函数称为GetPage@LSN。

Layer Files and Non-Overwriting Storage


##### Log-as-the-Database with Multi-Version Storage and Filtered Replay 

在LogDB-MV(图5)中，当GetPage@LSN请求到达存储节点时，在返回页面之前，它会重放所有日志直到所请求的LSN。但是，这个过程可能导致大量浪费的xlog重放，因为一些xlog可能与请求的页面不相关。

在存储节点中引入了“快速扫描”过程，以加速“Version Map”的更新，从而过滤GetPage@LSN请求中不必要的xlog重放。

例如，如果有一个请求用LSN 200检索页a，并且“Quick Scan”进程确定页a的最后一个相关xlog是LSN 150，那么GetPage@200将只等待replay process重放到LSN 150(而不是200)，然后返回该页。

##### Log-as-the-Database with Multi-Version Storage and Smart Replay (LogDB-MV-SR)

虽然LogDB-MV-FR架构可以跳过许多不必要的xlog来提高性能，但是仍然有一些不相关的xlog会被重放以服务GetPage@LSN。例如为了获得目标页面的第二个版本，replay process需要重放与目标页面无关的第一个xlog

有另一种日志重放方法(在PolarDB和Neon中使用)，我们称之为“Smart Replay”。只重放与请求页面相关的必要日志，不会重放任何不必要的日志。不同的进程可以并行重放不同页面的版本列表。


##### Understanding the Performance Implications of Storage-Disaggregated Databases 实现结论

16 线程，96G的Sybench测试：
对于轻工作负载，每个SysBench执行之间有30秒的空闲时间。相比之下，在连续SysBench运行之间，繁重的工作负载没有空闲时间。

![](./images/Screenshot%202024-07-23%20at%204.29.48%E2%80%AFPM.png)

- 存储分解的性能开销 （本地存储 vs RemoteDisk）
  - 使用SSD时，存储分解 **如果在没有缓冲的情况下应用** 会显著降低读取（16.4倍）和写入（17.9倍）的性能。这是由于与本地存储相比，通过网络访问远程存储的速度较慢。
- 缓冲对性能的影响 （本地存储 vs RemoteDisk）
  - 对于读取：80%命中率将分散数据库和非分散数据库之间的性能差距缩小到1.8倍。
  - 对于写入：由于需要在事务提交时将日志发送到存储，因此即使使用较大的缓冲区，缓冲也不会显著提高性能。

![](./images/Screenshot%202024-07-23%20at%204.34.05%E2%80%AFPM.png)

- 日志即数据库设计的有效性 （RemoteDisk vs LogDB）
  - 对于轻工作负载中的读写，没有显著的性能改进。
  - **对于繁重的工作负载，可以实现2.58倍的写入性能提升**，在使用80%缓冲区时。
    - 图10d显示，与Remote Disk架构相比，LogDB显著提高了写性能。这是因为，在繁重的工作负载下，Remote Disk架构中的本地缓冲区往往会因脏页而饱和，从而没有足够的时间在后台刷新这些页面。因此，当新的事务到达时，Remote Disk架构必须通过网络发送数据(由于刷新脏页)和日志。
    - 轻负载情况下，有足够的空闲时间在后台清理和刷新脏页面，导致写性能没有区别

![](./images/Screenshot%202024-07-23%20at%204.34.39%E2%80%AFPM.png)

- 对于写后读场景(写大批量数据，然后立即读) 分析日志重放对读取的影响 图13
  - 随着批量插入持续时间的增加，LogDB 的性能显着下降，而RemoteDisk架构的性能保持稳定。具体来说，当批量插入持续 5 分钟时，LogDB 落后于RemoteDisk 20.3%。
  - 多版本页面时，差距会扩大，插入持续时间为 5 分钟时，LogDB-MV 将比RemoteDisk损失 66.2%
    
- LogDB vs LogDB-MV 对比
  - 读无影响，只读工作负载不需要重放日志
  - 图14b显示了没有优化时，LogDB-MV 明显的性能下降。这是因为支持多个版本会导致性能开销的增加，包括xlog重放开销的增加(因为单个xlog现在可以对应多个页面)和在向RocksDB插入页面期间增加的开销(考虑到多版本导致页面数量的显著增加)。
  - 图14d显示，LogDB-MV的写性能优于LogDB（37%），说明多版本存储引擎提高了写性能。原因在于，在多版本页面中，页面更新不是原地更新的。因此，它自然地解决了“torn page write”问题，从而减少了网络上的数据传输。

- 日志回放效果影响  LogDB-MV、LogDB-MV-FR 和 LogDB-MV-SR  对比
  - 只读负载无影响，不需要回放日志，回放xlog 越少，存储节点负载越低，从而写入性能更高，GetPage@LSN 请求延迟更低
  - LogDB-MV-FR 的写入吞吐量大约是 LogDB-MV 的 1.5 倍
  - LogDB-MV-FR 的 GetPage@LSN 请求与 LogDB-MV 相比，节省了大约 72.6% 的等待 xlog 重放时间
  - LogDB-MV-SR 的写入吞吐量大约是 LogDB-MV-FR 的 1.6 倍
  - LogDB-MV-SR 仅为 LogDB-MV-FR 等待的 LSN 的 53.6%


### OLAP

- Snowflake
  
  - 存储层
    - 基于S3的高可用和持久行
      - 慢，但是廉价和可用
      - 计算节点依赖缓存
    - Partition table into file
      - 文件大小 16MB
    - PAX hybrid columnar storage format
    - 所有计算节点可以访问所有存储数据
  - 计算层
    - Virtual Warehouse (VW)，MPP
    - Elasticity
  - 控制层
    - 元信息，计划，访问控制

  - 完全的存算分离架构，以 SaaS 服务的形式去提供分析型数据库服务，并且使用了S3对象存储作为实际存储，成本较低。使用 Local Disck 作为热数据 Cache，兼顾性能，但一定程度上限制了弹性
  - 完全依赖 SaaS 服务设计，在国内做一些定制化部署时很难做一些裁剪和适配。

- Amazon Redshift
  - 存算分离架构,可以利用 AQUA 将算子下推到硬件中实现查询加速，还可以利用 CaaS 进行查询加速
  - 不足仍是架构定制化程度过高，难以复制
- databend

- starrock

- tiflash

- rocksdb cloud [存算分离](../../database_system/rocksdb/rocksdb.md)


### 组件

广泛的架构组件设计

- Compute Node 计算节点（leader,workers）
- cache 数据缓存
- metadata 元信息
  - 文件管理
- Storage Node 存储节点（可选）
  - 数据过滤
  - 共享存储数据管理:compact 
  - eg. TiFlash Write Node
- 共享存储（OSS,HDFS）、文件系统
  - 冷数据 parquet 文件存储格式 

### 思考

- 存储引擎的必要性与位置
  - 数据过滤计算，访问文件，写入处理
  - 单独服务 （tiflash） or 计算存储一体
- 对象存储中的数据内容
  - 索引，元数据
    - 多版本
  - block 大小，segment文件块 or 列文件块
  - 原始segment or parquet
  - 移除WAL(副本依赖共享存储)


### 系统设计

一个类OLAP系统存算分离架构设计

![](./images/obs-arch.png)

- agent 采集节点
- frontend 前端， web页面 
-  metastore 元信息缓存 （S3 上所有文件信息、分片等）
- ingester 写入节点
- query 查询节点，无状态
  - 查询数据  ingester 上 L0 级别信息 + 共享存储上持久化信息
- cache 缓存
  - 共享存储上持久化缓存的文件块

#### 写入流程
1. ingester 接收写入请求，并写入本地和副本
  a. 可以考虑 openDAL  库写文件
  b. 依然支持多副本、WAL 容错	
2. 后台异步将Level > 0 级别的数据flush到对象存储
  a. Level > 0 的数据是经过合并的数据
3. 并更新元信息


#### 读取
query 节点处理查询请求，无状态节点，都可以作为协调者节点，接收查询请求，将任务提交到其他节点处理，但是支持缓存 共享存储上的文件块。

当前并行执行框架是基于volcano 插入 xchange 的并行执行模型，而Morsel-Driven numa-aware并行执行模型，将计算任务迁移到数据位置，并且对后续算子处理缓存数据友好。为了避免缓存数据失效，split 切分时，也需要注意节点是否有数据缓存。

查询请求中Scan 请求（包括下推的filter）将被分发2次
- ingester 只搜索 L0 数据，并查询过滤条件并返回
- query 节点，根据磁盘缓存数据，按照数据亲和执行task
  - 无数据缓存、单点任务过多，其他query节点将远程读共享存储数据并过滤
非Scan执行算子，只会调度query节点执行（减少ingester节点读压力）

#### 数据缓存
缓存共享存储上的数据，应当优先将索引，元信息等缓存，其次数据文件块。
缓存淘汰策略  FIFO-like 可能比 LRU-like 更好

#### 共享存储上文件设计
L0文件的compact后的文件
- 相关索引文件
- 数据文件分块，尽量按列
- 数据文件块索引

优缺点
优点：
- 对原来sharenothing架构调整小，基本只是扩展，改动相对小
- 分离了较大冷数据查询对写入的压力 （需要测试）

缺点：
- 写入方面基本维持原来逻辑，热数据实时数据的查询，可能依然对写入性能提升改进不大

ps: 该架构实际已经有实现：durid [[笔记] Druid: A real-time analytical data store](https://fuzhe1989.github.io/2020/11/21/druid-a-real-time-analytical-data-store/) 其分为 实时节点（Real-time Nodes），负责服务实时读写请求，新写下去的数据立即可读。这些节点只负责最近很短时间段的数据，且定期会把这些数据发给历史节点。


## REF

### 学术

- [cs848-w2024](https://ozsu.github.io/cs848-w2024/Schedule.html) slides 推荐

- [SIGMOD23_DisaggregatedDB_Slides](https://www.cs.purdue.edu/homes/csjgwang/pubs/SIGMOD23_DisaggregatedDB_Slides.pdf)
  
  - [papaer](https://www.cs.purdue.edu/homes/csjgwang/pubs/SIGMOD23_Tutorial_DisaggregatedDB.pdf)

- [ASPLOS2020 Hailstorm: Disaggregated Compute and Storage for Distributed LSM-based Databases 笔记](https://zhuanlan.zhihu.com/p/381280016)
  
  - 计算/存储资源的解耦合
  - 存储池化（small block（1MB））
    - 在各shard下的storage engine引入一层shared-storage的存储层进行池化，分摊了io开销和消除io热点，消除数据倾斜和单机disk的瓶颈
  - Compaction/flush任务的均衡调度
    - 提供task调度功能，将compaction/flush任务根据io/cpu负载在不同分片中进行调度
    - HailStorm Agent类似于一个compaction/flush任务的scheduler，把io/cpu intensive的task分发到利用率比较低的节点上
    - 计算层的compaction offloading用于减缓cpu load
  - 基于文件系统操作（FSClient）
    - storage engine中对compaction/flushing是对文件进行操作的
    - 文件系统提供如mmap 内存映射方法
  - 元数据存储
    - 每个文件通过全局唯一的uuid来识别
    - 每个文件的block顺序存储，方便定位
    - 每个hailstorm client存储着文件路径和uuid的映射（全局元信息）
  - 读优化
    - 读取比写入时，block的粒度会更小，来减少传输的latency和读放大
    - compaction/flush操作则使用默认的block粒度来保证写入性能
  - 容错
    - LSM用WAL实现崩溃恢复保证数据一致性
    - 分布式数据库通过跨机器/跨机架/跨AZ/跨数据中心的备份来保持高可用
    - rack： RAID保证冗余
    - 文件的metadata存储在本地（bad, pg,mysql 主备，etcd等）

### 实践

- [ClickHouse 存算分离架构探索](https://zhuanlan.zhihu.com/p/357451583) 
- [ClickHouse 存算分离改造：小红书自研云原生数据仓库实践](https://zhuanlan.zhihu.com/p/654858066)

- [全新存算分离架构——[SIGMOD2021] PolarDB Serverless: A Cloud Native Database for Disaggregated Data Centers 笔记](https://zhuanlan.zhihu.com/p/382109937)

- [存算分离/DB on K8s 论文/blog收集](https://zhuanlan.zhihu.com/p/377755864)

- [存算分离下写性能提升10倍以上，EMR Spark引擎是如何做到的？](https://zhuanlan.zhihu.com/p/272202352) 

- [TiFlash 存算分离架构与 S3 支持](https://docs.pingcap.com/zh/tidb/stable/tiflash-disaggregated-and-s3#tiflash-%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB%E6%9E%B6%E6%9E%84%E4%B8%8E-s3-%E6%94%AF%E6%8C%81)
  
  - 存算分离架构中，TiFlash 原有进程的不同部分的功能，被拆分到两种不同的节点
    - TiFlash Write Node
      - 负责接收 TiKV 的 Raft logs 数据，将数据转换成列存格式，并每隔一小段时间将这段时间的所有数据更新打包上传到 S3 中
      - 管理 S3 上的数据，比如不断整理数据使之具有更好的查询性能，以及删除无用的数据等
    - TiFlash Compute Node 
      - 负责执行从 TiDB 节点发过来的查询请求
        - 首先访问 Write Node 以获取数据的快照 (data snapshots)
        - 然后分别从 Write Node 读取最新的数据（即尚未上传到 S3 的数据）
        - 从 S3 读取剩下的大部分数据
      - Compute Node 利用本地磁盘（通常是 NVMe SSD）来作为数据文件的缓存，从而避免相同的数据反复从远端（Write Node 或者 S3）读取，以提高查询性能
        ![tiflash](./images/tiflash-s3.png)

- [StarRocks 存算分离 Data Cache 二三事](https://zhuanlan.zhihu.com/p/695673099)
  
  - Cache V1: File Cache,  LRU 淘汰, 同时写入对象存储和本地 Cache
    - 缺点：
      - 空间效率低， 可能访问只有部分列数
      - Cache Miss 时代价大
      - 淘汰效率低， 在本地文件系统上 list 上百万甚至千万的文件
      - Cache Disk 不均衡，将某个分区映射到特定的磁盘，数据分区大小不均衡导致磁盘使用不均
  - Cache V2: Block Cache（StarCache ）
    - 以特定大小 Block （典型大小如 1MB）为缓存单位的新型 Cache 系统
    - 80% 磁盘空间阈值

- [StarRocks 存算分离 Compaction 原理](https://zhuanlan.zhihu.com/p/687354691) 

- [StarRocks 存算分离成本优化最佳实践](https://zhuanlan.zhihu.com/p/683731194)

- [StarRocks 存算分离最佳实践，让降本增效更简单](https://zhuanlan.zhihu.com/p/669724897)
  - 对象存储在 IO 延迟上相比于本地盘确实要高出 1 ~ 2 个数量级，但对象存储胜在高吞吐能力，AWS S3 单 Bucket 的最高吞吐甚至能达到数百 GB
  - 256 线程 * 单次1MB  高并发下数据实时入湖，使用事务提交 Batch 优化，可以达到 90,880 KB/s， 直接写 14，592 KB/s

- [兼顾降本增效，StarRocks 3.0 关于存算这对CP分离的最佳"姿势"](https://zhuanlan.zhihu.com/p/681149648)
  - 在存算分离架构下将 BE 升级为了 CN（Compute Node）,做到无状态
  - StarManager 又整合了 Shard Manager、Service Manager、Worker Manager、FileStore Manager、LogStore Manager 的能力，来更好地管理 tablet。
  - 将 BE 中的 tablet metadata 和 segment data 都存储到了外部存储, 而没有放到FE
  - 挑战
    - 异构存储系统的支持
    - 如何降低 IO 延迟
      - 正常 Nvme SSD 的 IO 延迟可达到100 us 以内
      - HDD 的延迟也可以保持在 5 ms 以内
      - Distributed Storage 的 IO 延迟大多数是在 1～100 ms，甚至想保持在 10 ms 都很不容易
    - 存储元数据的一致性如何保障
      - 写入后是否能立刻读到，还有写入的文件是否能立刻 list 出来
    - 计算与存储缓存调度， 缓存和计算的调度
    - Compaction 和 GC
  - 关键技术设计
    - 缓存
      - 利用本地盘和内存将远程存储拉回的数据缓存下来，加速后续查询
      - 缓解读 IO 延迟
    - 不可更改文件，多版本控制
    - 写幂等
      - 在某个节点操作 tablet 的时候可能丢失和 FE的心跳，处于假死状态，FE 会将任务交给另外一个节点，就会发生多个节点同时操作同一个 tablet，导致数据损毁。通过写幂等性就可以保证数据的正确性，即文件在多次完全替换情况下还能保证是正确的。
    - 多版本管理



- [兼顾降本与增效，我们对存算分离的设计与思考](https://zhuanlan.zhihu.com/p/630277812) StarRocks
  
  - 构建了一个统一的存算分离平台——StarOS， StarOS 的调度中枢负责计算任务调度
    - 基于 StarOS, 构建 StarRocks
    - FE
      - Compaction：将用户的历史版本合并为一个更大的版本以提高查询性能
        - 得益于存算分离的数据共享能力，FE 可以选择将 Compaction 任务发往任意 BE 节点执行
        - TODO 专用集群，避免干扰集群
    - BE/cache
  - 存储
    - 后端存储方式
      - 兼容 AWS S3 协议的对象存储系统
      - 传统数据中心部署的 HDFS
    - StarRocks 存算分离数据文件格式与存算一体相同，数据按照 Segment 文件组织，StarRocks 各种索引技术在存算分离表中也同样复用
    - 数据多版本
      - 每个数据版本包含 Tablet Meta 和 Tablet Data 文件，并且都写入后端对象存储
        - TabletMeta 文件内记录了该版本所有的数据文件索引
        - Tablet Data 文件仍然按照 Segment 文件格式组织
      - BE 节点需要访问某个Tablet 时，会先根据版本号从后端存储加载对应的 Tablet Meta 文件，然后再根据索引访问相应的数据文件
  - 缓存
    - 使用磁盘（Local Disk） 来缓存热点数据
  - 对比
    - 写入：随着并发提升，存算分离表的写入吞吐仍在不断提升，直到最终达到了 BE 节点的网络带宽瓶颈
      - 延迟？实时性
    - 查询
      - 存算分离版本在缓存完全命中情况下，其总体性能已经追平存算一体（428s VS 423s）1.01X
      - 完全访问冷数据情况下，通过预读等优化手段，也很好地将冷数据查询性能衰退控制在一个合理范围内（668s VS 428s）  1.57X


- [实测结果公开：用户见证 StarRocks 存算分离优异性能！](https://zhuanlan.zhihu.com/p/644150614)

- [万字长文|从AIGC典型客户实践揭秘云原生向量数据库内核设计与智能创新](https://mp.weixin.qq.com/s/KPN-JoICYnTf-wyB-Rmoig) 向量数据库的存算分离
  
  - 迭代过程：
    
    - 在PostgreSQL上实现了类似pgvector的向量索引插件，支持了高维向量的高效检索，支持了向量数据的实时更新等基础功能
      
      - 基于段页式存储的HNSW索引，3次遍历图索引
      - 问题：段页式存储带来的加锁访问开销占据了整个执行时间的1/3，大量随机图查询造成大量Shared Buffer页面申请淘汰
    
    - 基于Huge-Block的自研向量索引
      
      - 把向量索引的数据按照1GB大小为一块来申请
    
    - Relyt-V架构
      
      - DWSU 数仓服务单元 包含多种DPS数据处理服务集群，DPS共享一份数据，一个DPS为读写集群，其它DPS为只读集群
        
        - Hybrid DPS提供了数据实时写入，实时分析的能力，Extreme DPS提供了极速Ad Hoc查询，交互式分析能力，Spark DPS提供了离线分析，以及Vector DPS提供向量和全文的检索能力
      
      - Vector DPS
        
        - 典型的数据库架构，包含各种SQL计算和存储的实现， 支持的索引，包含B-tree、全文、JSON和向量索引
        
        - 计算层是PostgreSQL集群，负责向量的写入和查询
        
        - 中间层的Block Service提供PostgreSQL的Page回放和读服务，Log Service提供WAL日志的持久化服务，Index Service提供向量索引的构建服务
          
          - 索引的服务化能力，提供索引的异步构建能力，同时索引构建不会影响上层计算的读写请求
        
        - 底层是对象存储，提供数据的持久化能力。
        
        - 存算分离改造
          
          - 从它的WAL日志做了Hook，把WAL日志路由到Log Server，并通过Paxos协议保证WAL日志的高可靠
          - PostgreSQL读写Page做了Hook，读Page路由到Block Server，Block Server从Log Server拉取WAL日志会回放成Page
          - Log Server和Block Server定期会把自己的数据同步到对象存储持久化
          - 持久化完成后，Log Server和Block Server就可以安全的清理自己的WAL日志和本地文件
            ![](./images/aigc1.webp)
        
        - 弹性Serverless
          
          - Block Server的迁移（负载），PG重试读取Page
          - Log Server 通过多副本的增减实现副本跨节点的迁移，有状态的网络中断、进程重启，通过QEMU实现的虚拟机实现进程的迁移，通过VXLAN的网络虚拟化能力，解决迁移过程中网络不中断的问题
        
        - 向量索引服务化
          
          - LSM的向量索引存储引擎
          - 按需拉起Index Build Service，通过从对象存储同步Vector数据来实现向量索引的构建
          
          ![](./images/aigc2.webp)


- [Elasticsearch 存算分离技术浅析与最佳实践](https://cloud.tencent.com/developer/article/2301790) 基于 es 的可搜索快照

- [Elasticsearch 存算分离功能 POC 方案](https://cloud.tencent.com/developer/article/2354578)
  - 支持将索引数据下沉、卸载到远程共享存储，副本和主分片共享一份数据，本地仅保留少量meta数据，降低磁盘占用。

- [无状态-Elasticsearch](https://www.elastic.co/search-labs/blog/stateless-your-new-state-of-find-with-elasticsearch)
   - 通过分离索引和搜索责任来重新构建Elasticsearch
     - 索引层 （index + translog）
     - 搜索层
   - 效果
     - 索引吞吐量提高75% 
        - CPU密集型索引操作只需要在索引层上发生一次，然后将结果段发送到对象存储区
        - 省略合并Lucene段
        - 分离读请求
      

- Milvus
  - [Milvus 存算分离系列-1：milvus架构简介](https://zhuanlan.zhihu.com/p/654044762)  描述基本读写流程，datanode 分片 segment 上传 oss，query node 下载  
  - [Milvus存算分离系列-2: target机制](https://zhuanlan.zhihu.com/p/654283186) 存算分离架构下保证数据（近，秒级别）实时可见&&数据完整性
    - 数据实时更新
      -  target 机制，可读segment的集合target，mixCoord会定期检查target状态，当发现存在新的target的时候，就会命令compute节点load新的segment以供client查询，从而保证新写入的数据能被client正确查询到
        - compact （target的更新：load、offload） 
          - PS：Milvus  由于在对象存储之后compact导致的问题，对于OLAP,时序数据库不支持更新情况下，完全可以先合并小segement 在上传到oss，不需要面临数据一致性问题，即 l0 数据在本地，l1级别数据在oss
        ![](./images/v2-f5d4e30414bbab35a72eacd6536d6f55_b.jpg)
        - target 状态更新机制
      - 数据新鲜度
        - way1：存算双读，将存储节点的热数据和计算节点上synced数据
          - bad：存储节点会受到计算负载的影响
        - **存算双写**: 同一份数据，既写入存储节点，又写入计算节点，但数据准备好后，再移除本地growing segment
        ![](./images/v2-fac4bb1199c0eb62bd13d4c00a7121dc_b.jpg) 
          - bad: 数据冗余, 增加计算节点内存开销
          - 选择双写原因：向量的查询计算量很大，cpu资源比较紧张，而存储节点的cpu需求则比较低，计算需求&&资源都集中在计算节点
        ![](./images/v2-33e4fcde4a87706ee5046c000a08b6d0_b.jpg)


- [ClickHouse 存算分离架构探索](https://cloud.tencent.com/developer/article/2109545)

- [Kudu: Storage for Fast Analytics on Fast Data](http://47.241.45.216/2022/10/30/Kudu-Storage-for-Fast-Analytics-on-Fast-Data/)

- [云原生存算分离数据库Neon的架构决策](https://zhuanlan.zhihu.com/p/656131188)

- [AlloyDB for PostgreSQL under the hood: Intelligent, database-aware storage](https://zhuanlan.zhihu.com/p/522460906)

- [理解Storage-Disaggregated数据库中的设计原则对性能的影响](https://zhuanlan.zhihu.com/p/712684045)
- [Understanding the Performance Implications of Storage-Disaggregated Databases](https://muratbuffalo.blogspot.com/2024/07/understanding-performance-implications.html) 推荐