# 大数据系统-鉴赏

鉴赏系列：

回答一下几个问题：

- 使用场景，起源问题

- 核心原理、架构
- 优、缺点，竞品比较

[TOC]



## 1. Hadoop

### 1.1 目标场景

Hadoop是Google File System理念的开源版本，其分布式文件系统HDFS，依然是当前大数据系统生态的事实基础。MR的批处理模型，被其他大数据计算引擎如Spark所吸收，仍是重要的计算模式。（资源管理器YARN单独，hadoop自身的MR已经退环境了，不在此处谈论）

**GFS设计目标**（文件系统）

- 性能（吞吐）
- 可伸缩性
- 可靠性
  - 面临的环境
    - 组件失效是常态（廉价商业PC）(bug,硬件,人为)
- 可用性
- 适应google的应用负载（读写模式）和硬件环境
  - 大量的流式读，少量的随机读
  - 大文件普遍，数个G大小，数百万个文件
  - 修改操作，大部分是文件尾追加，而非覆盖
  - 应用原子操作文件系统API（高并发）



### 1.2 架构与原理

![](大数据系统鉴赏/HDFS-Architecture_20191001204831887472.png)



- Client
  - 控制流与数据流解耦
    - 与namenode元信息交互
    - 与datanode的读写IO
  - 就近访问
- NameNode
  - NameNode 负责管理文件系统命名空间，控制客户端对文件的访问。此外，它还执行诸如打开、关闭和重命名文件和目录之类的任务。
    - **FSImage –** HDFS 元数据的时间点快照。它包含文件权限、磁盘配额、修改时间戳、访问时间等信息。
    - **Edits log- ** 它包含对 FSImage 的修改。它记录增量更改，例如重命名文件、将数据附加到文件等。
  - Secondary NameNode
    - HA节点，容错，通过应用Edits log进行同步。
- DataNode
  - 数据的存储节点。
  - 一个文件被分成多个数据块并存储在一组DN上。
    - 块
      - 最小的存储单元。默认128MB/256MB。
        - 如果设计过小，导致元信息膨胀。
        - 扩展：关于数据分布有一致性hash，统一公式定位数据块，元信息不随数据量膨胀。但是节点有变更相关问题，数据迁移。（结合？)
  - 容错
    - 副本因子3



### 1.3 优缺点

HDFS存储引擎的优点

- 大数据集的存储
  - 传统的 RDBMS 无法存储大量数据。可用 RDBMS 中的数据存储成本非常高。因为它会产生硬件和软件的成本。（TB，PB级）
  - 可扩展，低成本
- 处理不同格式的数据
  - RDBMS 能够以结构化格式存储和操作数据。但在现实世界中，我们必须以结构化、非结构化和半结构化格式处理数据。
- 容错
  - 三副本

缺点：

- 小文件问题
  - 小文件小于hadoop块大小的文件。大量的小文件带来NN元信息管理负担。
- 



**缺点**

**计算框架MRv1的缺点**

- 扩展性问题，jobTacker同时具有资源管理和作业控制功能，导致成为瓶颈制约扩展性
- 可靠性，mastr/slave ，单点故障。
- 资源利用率低，槽位机制，map，reduce不共享
- 无法支持其他计算框架，内存计算，流计算，迭代计算，只用于磁盘的离线计算。



### REF

- [data-flair: Hadoop Tutorial](https://data-flair.training/blogs/hadoop-tutorial/) 推荐



## 2.Spark

### 2.1 目标场景



### 2.2 基本原理

#### 2.2.1 执行流程

![](大数据系统鉴赏/Snipaste_2021-06-15_03-18-03.png)

具体运行流程如下：

1. SparkContext 向资源管理器注册并向资源管理器申请运行Executor

2. 资源管理器分配Executor，然后资源管理器启动Executor

3. Executor 发送心跳至资源管理器

4. SparkContext 构建DAG有向无环图

5. 将DAG分解成Stage（TaskSet）

6. 把Stage发送给TaskScheduler

7. Executor 向 SparkContext 申请 Task

8. TaskScheduler 将 Task 发送给 Executor 运行

9. 同时 SparkContext 将应用程序代码发放给 Executor

10. Task 在 Executor 上运行，运行完毕释放所有资源

Spark Application中可以因为不同的Action触发众多的job，一个Application中可以有很多的job，每个job是由一个或者多个Stage构成的，后面的Stage依赖于前面的Stage，也就是说只有前面依赖的Stage计算完毕后，后面的Stage才会运行。

Stage划分的依据就是宽依赖，像reduceByKey，groupByKey等算子，会导致宽依赖的产生。

#### 2.2.2 spark on yarn

![](大数据系统鉴赏/Snipaste_2021-06-15_03-43-40.png)

![](大数据系统鉴赏/Snipaste_2021-06-15_03-45-46.png)

application的spark-context在本机。



shuffle write：stage，task结果输出到本地文件系统。

shuffle read：stage读取依赖的stage的数据

本地读，远程读。（bar work）





### REF

- [Spark底层执行原理详细解析](https://mp.weixin.qq.com/s/qotI36Kx3nOINKHdOEf6nQ)

- Spark技术内幕 深入解析spark内核架构设计与实现原理-张安站



## 3.Yarn

### 3.1 场景

MR v1的改进版本，独立的资源管理框架，可以支持不同的计算框架，MR，Spark等。





### 3.2 Yarn原理

#### 3.2.1 基本架构

![](大数据系统鉴赏/YARN-working-1.png)

![](大数据系统鉴赏/Snipaste_2021-06-15_04-01-33.png)



#### 3.2.2 yarn工作流程

![](大数据系统鉴赏/Snipaste_2021-06-15_04-06-49.png)

1. 步 骤 1 用 户 向 YARN 中 提 交 应 用 程 序， 其 中 包 括 ApplicationMaster 程 序、 启 动ApplicationMaster 的命令、 用户程序等。
2. 步骤 2 ResourceManager 为 该 应 用程 序 分 配 第 一 个 Container， 并 与 对应 的 NodeManager 通信， 要求它在这个 Container 中启动应用程序的 ApplicationMaster。
3. 步 骤 3 ApplicationMaster 首 先 向 ResourceManager 注 册， 这 样 用 户 可 以 直 接 通 过
   ResourceManage 查看应用程序的运行状态， 然后它将为各个任务申请资源， 并监控它的运
   行状态， 直到运行结束， 即重复步骤 4~7。
4. 步骤 4 ApplicationMaster 采用轮询的方式通过 RPC 协议向 ResourceManager 申请和
   领取资源。
5. 步骤 5 一旦 ApplicationMaster 申请到资源后， 便与对应的 NodeManager 通信， 要求
   它启动任务。
6. 步骤 6 NodeManager 为任务设置好运行环境（包括环境变量、 JAR 包、 二进制程序
   等） 后， 将任务启动命令写到一个脚本中， 并通过运行该脚本启动任务。（与ArgoDBBAR的导入导出脚本，异曲同工）
7. 步骤 7 各个任务通过某个 RPC 协议向 ApplicationMaster 汇报自己的状态和进度， 以
   让 ApplicationMaster 随时掌握各个任务的运行状态， 从而可以在任务失败时重新启动任务。
   在应用程序运行过程中， 用户可随时通过 RPC 向 ApplicationMaster 查询应用程序的当
   前运行状态。
8. 步骤 8 应用程序运行完成后，ApplicationMaster 向 ResourceManager 注销并关闭自己。



#### 3.2.3 容错

app master 容错：RM 心跳检查，重启

container 容错：RM+AM处理

RM容错：HA



### 3.3 优缺点

### 3.3.1 相比MRv1优点

-  资源利用率提升。不同框架计算引擎，可以共用一套物理集群，运维成本降低。
- 数据共享，不同框架共用hdfs资源数据。
- 





### REF

- hadoop技术内幕：深入解析yarn架构设计与实现原理-董西成
- 





## 4.Delta Lake

### 4.1 场景

Delta Lake, 在 S3、ADLS、GCS 和 HDFS 等现有存储系统之上构建[Lakehouse 架构](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf)。 

数据湖/流批处理（取代lambda架构）。

![](大数据系统鉴赏/Delta-Lake-marketecture-0423c.png)

主要特性：

- ACID 
  - 对于多个数据管道访问数据湖，可以保证ACID事务，serializability，[Diving into Delta Lake: Unpacking the Transaction Log](https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html)
- 可扩展的元数据处理
  - 在大数据系统中，元信息本身也是大数据（hadoop namenode的痛苦），Delta Lake将元信息分布式存储（或者说本地化，元信息与数据共址），使其可扩展。
- 提供时间旅行
  - 元信息管理的好处
- 开放格式
  - 数据使用parquet格式存储，易于接入hadoop 生态，并且高效压缩和编码。
- 统一批处理和流处理
  - 同时是数据流/数据表
- 支持Update、delete和merge
- 完全兼容Spark API
  - spark使用delta表非常简单
- 支持其他连接器：rust，python，hive，presto等



最佳实践：

COMCAST： PB级任务，640 server node -> 64; ETL job数量从 84 -> 3。



### 4.2 基本原理

Delta lake 采用将事务日志和元数据直接存储在云存储对象（论文针对的云对象存储，实际就是表）中，并在对象存储操作上使用一组协议来实现可序列化。

![](大数据系统鉴赏/Snipaste_2021-07-25_13-47-58.png)



#### Delta表

表= 操作的集合 （当前的元信息、文件列表，事务列表，版本）

操作：

- 元信息变更（name，schema，分区等）
- 添加文件（可选，统计信息）
- 删除文件



#### 原子性

所有的变更，都会以有序，原子性的提交。

`.json` 文件是日志记录对象。包含一系列对的表操作，将其从一个状态，转移到下一个状态。如图所示，（对数据信息了变更，删除文件，增加文件）

![](大数据系统鉴赏/Snipaste_2021-07-25_13-41-45.png)

为了改进读取大量commit log文件生成当前表状态的性能，引入检查点`.parquet`文件，定期存储之前所有非冗余的操作集合（parquet格式，高压缩的日志记录）。



采用乐观并发控制，提交时检测commit log文件是否冲突，冲突，读取变更，重试。

其他优化：Z-order格式，统计信息维护。



### 4.3 优缺点

传统lambda架构缺点：

- 架构复杂，批流处理分别引擎
- 并发处理，数据错误，验证
- 单点的元信息获取
  - 从hive metastore查询百万级的分区信息，namenode查询每个分区下的大量文件，千万，亿级
- 延迟数据需要再全部处理一遍。

优势：

- 是支持单表ACID的插件库，方便集成到现有hadoop生态。
- 支持同时的读写，并且保证数据一致性
- 高性能的大表读取
  - 元信息在本地，可以分布式处理
  - 向量化parquet读
- 错误写入可以回滚和删改
- 同时处理历史数据
- 处理迟到的数据，无需推迟下个阶段的数据处理
  - 通过update/merge into操作

缺点:

- 只是单表跨行事务



### REF

- [官网](https://delta.io/)
- [github: delta](https://github.com/delta-io/delta)
- [delta-rs](https://github.com/delta-io/delta-rs)  rust访问delta lake 表
- [VLDB 2020 论文：delta lake: high-performance acid table storage over cloud object stores](https://databricks.com/wp-content/uploads/2020/08/p975-armbrust.pdf)
- [CIDR 2021：Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf)
- [slide:Delta Lake：Open Source Reliability for Data Lake with Apache Spark 李潇](https://www.slideshare.net/XiaoLi8/delta-lake-architecture-delta-lake-apache-spark-structured-streaming)



## 5.HBase

### 5.1 场景

hbase是bigtable的开源版本，而bigtable是google为了存储大规模结构化、半结构化数据如（url,文档，图像，日志文件、二进制文件）的数据库系统。

提供随机访问的存储和检索。

行即一致性，水平扩展。

高可用（hdfs提供多副本），高性能（KV存储，列存，方便点查询，写入，数据量增长，时间增不明显），可扩展。

起源: google 互联网的搜索

根据关键词搜索，需要建立索引。索引根据MR计算出来。不停的通过爬虫抓取互联网上的文档，并存储进系统，单个文档能够插入，更新。用户搜索时，根据索引，查到到存储引擎中的文档。

场景：

- 抓取增量数据
  - 监控指标（用户的交互，服务流量等）
  - 遥测数据，存储用户计算机上生成的软件崩溃报告
  - 广告效果，点击流，待批处理分析，生成报表
- 内容服务
  - 存储各个用户生成内容，Twitter的贴子、图片等，并提供给自己和其他用户显示
  - URL短链，生成用户特征
- 信息交换
  - 短信系统
  - （hbase集群上千台，region管理，上下线问题处理。在星环处理20,30台的集群就容易出现各种问题）





### 5.2 基本原理

#### 5.2.1 基本架构

![](大数据系统鉴赏/Snipaste_2021-06-23_15-18-15.png)

- Hbase client
  - Shell命令行接口、原生Java API编程接口、Thrift/REST API编程接口以及MapReduce编程接口
  - 提供DML/DDL操作，表管理等
- zookeeper
  - 监控master servers，并宕机时选举新master
  - 系统核心元信息，RegionServer地址(hbase:meta)
  - 参与RegionServer宕机恢复：ZooKeeper通过心跳可以感知到RegionServer是否宕机，并在宕机后通知Master进行宕机处理。
  - 实现分布式表锁：HBase中对一张表进行各种管理操作（比如alter操作）需要先加表锁，防止其他用户对同一张表进行管理操作，造成表状态不一致。
- Master
  - 处理client管理请求，建表，修改表，权限，切分表、合并数据分片以及Compaction等
  - 管理RegionServer，RegionServer中Region的负载均衡、RegionServer的宕机恢复以及Region的迁移等
  - 清理过期日志以及文件，Master会每隔一段时间检查HDFS中HLog是否过期、HFile是否已经被删除，并在过期之后将其删除
- RegionServer
  - 响应用户的IO请求
    - WAL(HLog)：持久化，hbase集群主从复制
    - BlockCache：读缓存。
    - Region：数据表的一个分片，负载均衡的基本单位
      - Store：数量对应表中列族的个数，推荐IO特性相同的放在一个列族
        - MemStore:1个，LSMtree的内存实现结构。
        - HFile:1或多个，MemStore写满后（默认128M），异步刷新到磁盘的结构。定期compact合并成大的文件
- HDFS
  - hbase实际存储系统，HFile，HLog都会写入hdfs。



### 5.3 优缺点

优点：

- 存储容量大
  - 单表支持千亿行，百万列，数据容量达TB，PB级别
- 良好的扩展性
  - 数据存储节点Datanode，读写服务节点regionserver扩展容易
  - 当然还是存储单点，nn，master
- 稀疏存储
  - kv存储，允许大量列为null值，不占用空间
- 高性能
  - oltp场景，随机单点查询，小范围扫描性能好。大范围使用MR的接口。
- 多版本
  - 拥有时间戳，可以时间旅行的读取
- 支持过期
  - 自动清理过期数据
- hadoop生态
  - 存储直接使用hadoop，很容易接入其他系统



缺点：

- 聚合性能很差
  - group by，join
- 本身没有二级索引
  - 第三方Phoenix提供的二级索引功能
- 只支持单行事务
  - 第三方Phoenix提供的全局事务模型组件





竞品比较：

HBase与TiKV 对比。

功能。支持的存储。

成本(服务硬件需求)





### REF

- [HBase原理与实践-胡争](https://weread.qq.com/web/reader/632326807192b335632d09ckc81322c012c81e728d9d180) 微信读书网页版，微信刷码即可，**推荐分析原理解析时阅读**
- 《HBase实战》
- 



## 6. Hive



## 7.Presto

Presto是一个分布式的采用**MPP架构**的查询引擎，本身并不存储数据，但是可以接入多种数据源，并且**支持跨数据源的级联查询**。Presto是一个OLAP的工具，擅长对海量数据进行复杂的分析；但是对于OLTP场景，并不是Presto所擅长，所以不要把Presto当做数据库来使用。

Presto是一个低延迟高并发的**内存计算引擎**。需要从其他数据源获取数据来进行运算分析，它可以连接多种数据源，包括Hive、RDBMS（Mysql、Oracle、Tidb等）、Kafka、MongoDB、Redis等。



![](大数据系统鉴赏/Snipaste_2021-06-15_02-11-54.png)

额外:为了支持多源，也使用classloader方式，支持不同的数据源client，依赖的不同版本的jar包



TODO：比较星环联邦计算方案

spark 批处理架构，sql，谓词等下推（dblink技术，jdbc），数据源（oracle，cdh，TD等）



和Apache HAWQ（MPP和批处理融合）差异？

支持的数据源。

## 8.Impala

Apache Impala是采**用MPP架构**的查询引擎，本身不存储任何数据，**直接使用内存进行计算**，兼顾数据仓库，具有实时，批处理，多并发等优点。

提供了类SQL（类Hsql）语法，在多用户场景下也能拥有较高的响应速度和吞吐量。它是由Java和C++实现的，Java提供的查询交互的接口和实现，C++实现了查询引擎部分。



Impala支持共享Hive Metastore，但没有再使用缓慢的 Hive+MapReduce 批处理，而是**通过使用与商用并行关系数据库中类似的分布式查询引擎**（由 Query Planner、Query Coordinator 和 Query Exec Engine 三部分组成），可以直接从 HDFS 或 HBase 中用 SELECT、JOIN 和统计函数查询数据，从而大大降低了延迟。

Impala经常搭配存储引擎Kudu一起提供服务，这么做最大的优势是查询比较快，并且支持数据的Update和Delete。

## 9.Elasticsearch



## 10.Clickhouse

### 10.1 场景

#### BI 背景

传统BI问题：生成业务决策的各种报表，需要花费大量时间，实时性不足。

一些改进的措施，引入数据仓库的概念，汇总数据，避免信息孤岛（后来的，多样格式的数据，数据湖）。

- 对数据进行分层，通过层层递进形成数据集市，从而减少最终查询的数据体量；

- 提出数据立方体的概念，通过对数据进行预先处理，以空间换时间，提升查询性能；

新BI：轻量级，Excel文件也能分析。数据分析师角色增加，不再只是高策决策查看。数据分析的普及化（个人用户，查询自己的统计，网易云音乐，支付宝年度总结）。互联网用户对实时性的要求增加，立即得到。



#### OLAP 架构分类

多维分析: 立方体cube操作

- 下钻：从高层次向低层次明细数据穿透。例如从“省”下钻到“市”，从“湖北省”穿透到“武汉”和“宜昌”。
- 上卷：和下钻相反，从低层次向高层次汇聚。例如从“市”汇聚成“省”，将“武汉”“宜昌”汇聚成“湖北”。
- 切片：观察立方体的一层，将一个或多个维度设为单个固定值，然后观察剩余的维度，例如将商品维度固定为“足球”。
- 切块：与切片类似，只是将单个固定值变成多个值。例如将商品维度固定成“足球”“篮球”和“乒乓球”。
- 旋转：旋转立方体的一面，如果要将数据映射到一张二维表，那么就要进行旋转，这就等同于行列置换。

![](大数据系统鉴赏/Snipaste_2021-07-08_02-17-30.png)

实现这些操作的，OLAP架构：

- ROLAP，关系型OLAP。关系模型，数据模型常使用星型模型或者雪花模型。SQL查询。
- MOLAP，多维型OLAP。预先聚合结果，间换取时间，典型Kylin。（更新问题，数据存储空间膨胀问题，无法查询明细信息，下钻操作）
- HOLAP，混合架构OLAP。ROLAP和MOLAP两者的集成。

OLAP演进，大数据的兴起，ROLAP 关系数据库 -> Hive，SparkSQL。（批处理，大规模数据，实时性不足，亚毫秒级，并发）

MOLAP演进，利用MR，Spark作为计算引擎，加速cube的构建。聚合结果，存储在hbase等分布式数据库。

（仍未解决维度爆炸、数据同步实时性不高）

>## OLAP场景的关键特征[ ](https://clickhouse.tech/docs/zh/#olapchang-jing-de-guan-jian-te-zheng)
>
>- 绝大多数是读请求
>- 数据以相当大的批次(> 1000行)更新，而不是单行更新;或者根本没有更新。
>- 已添加到数据库的数据不能修改。
>- 对于读取，从数据库中提取相当多的行，但只提取列的一小部分。
>- 宽表，即每个表包含着大量的列
>- 查询相对较少(通常每台服务器每秒查询数百次或更少)
>- 对于简单查询，允许延迟大约50毫秒
>- 列中的数据相对较小：数字和短字符串(例如，每个URL 60个字节)
>- 处理单个查询时需要高吞吐量(每台服务器每秒可达数十亿行)
>- 事务不是必须的
>- 对数据一致性要求低
>- 每个查询有一个大表。除了他以外，其他的都很小。
>- 查询结果明显小于源数据。换句话说，数据经过过滤或聚合，因此结果适合于单个服务器的RAM中

#### ClickHouse的起源

俄罗斯的Yandex（搜索引擎），业务背景在线流量分析Yandex.Metrica（支持广告投放，用户行为分析）。

发展阶段

- 早期的Yandex.Metrica以提供固定报表的形式帮助用户进行分析，例如分析访问者使用的设备、访问者来源的分布之类。使用了MySQL数据库作为统计信息系统的底层存储和分析引擎的解决方案。
  - 关心写入和查询，不关系事务，使用MyISAM表引擎
  - B+树存储，多点并行，随机写入。产生大量磁盘碎片。
  - 经过许多优化，控制在26s内。但是数据越来越大（2011年，5800亿行）
- Metrage系统。
  - Key-Value模型（键值对）代替了关系模型。
  - LSM树代替了B+树。写入性能增强。顺序写。
  - 由实时查询的方式改为了预处理。MOLAP路线。
  - 结果：超过3万亿行的数据，超过60台服务器，查询性能1s以内。（解决了性能瓶颈）
  - 缺点，数据膨胀10-20倍。
- OLAPServer系统。
  - 需要支持自定义分析报告服务（ad-hoc查询）。无法预聚合。
  - 关系模型。
  - SQL。
  - 存储结构和索引。结合MyISAM和LSM树。索引文件和数据文件。索引，lsm树稀疏索引定位数据段。数据文件，数据段内有序。索引文件和数据文件按照列字段的粒度进行了拆分，每个列字段各自独立存储。（列存）
  - 缺点，只支持定长数值类型，无DDL支持。
  - 解决ad-hoc实时聚合的性能问题。
- ClickHouse系统。
  - ROLAP架构。实时聚合。
  - **Click**Stream + Data Ware**House**
  - 列存，SQL，实时。
  - 20万亿行，90%查询在1s内。

适合场景：

ClickHouse非常适用于商业智能领域（也就是我们所说的BI领域），除此之外，它也能够被广泛应用于广告流量、Web、App流量、电信、金融、电子商务、信息安全、网络游戏、物联网等众多其他领域。

不适合场景——OLTP事务性操作的场景：

- 不支持事务。

- 不擅长根据主键按行粒度进行查询（虽然支持），故不应该把ClickHouse当作Key-Value数据库使用。

- 不擅长按行删除数据（虽然支持）。



现实应用：

欧洲核子研究中心（CERN）将它用于保存强对撞机试验后记录下的数十亿事件的测量数据，并成功将先前查找数据的时间由几个小时缩短到几秒。



### 10.2 基本架构与原理

ClickHouse是单一角色节点的设计，但是支持分布式的集群部署。与kafka的选择相同。（全网暂无一张架构图，仅源码模块组织图）

![](大数据系统鉴赏/Snipaste_2021-07-08_23-06-14.png)

- Columns模块

  - ClickHouse按列存储数据，内存中的一列数据由一个Column对象（继承IColumn，例如ColumnString，ColumnArray）表示，定义了对数据进行各种关系运算的方法（create，insert，filter，getData）。Field对象代表一个单值，内部聚合了Null、UInt64、String和Array等13种数据类型及相应的处理逻辑

- DataTpye模块

  - 数据的序列化和反序列化。继承IDataType，获取各种数据类型的SerDe实例。

- DataStreams模块

  - ClickHouse内部的数据操作是面向Block对象进行，并且采用了流的形式。
  - 包含数据的类型及列的名称。Column、DataType。使用ColumnWithTypeAndName获得引用。
  - IBlockInputStream负责数据的读取和关系运算，IBlockOutputStream负责将数据输出到下一环节。
  - 实现类
    - 处理数据定义的DDL操作，DDLQueryStatusInputStream
    - 处理关系运算的相关操作，LimitBlockInputStream，JoinBlockInputStream
    - 处理表引擎访问，MergeTreeBaseSelectBlockInputStream（MergeTree表引擎）

- Storages模块

  - 表引擎。使用IStorage接口指代数据表。数据访问（指定列的原始数据）。
  - 不同的表引擎由不同的子类实现，例如IStorageSystemOneBlock（系统表）、StorageMergeTree（合并树表引擎）和StorageTinyLog（日志表引擎）等。 
  - IStorage接口定义了DDL，read和write方法。
  - 存储：mysql，pg，mergeTree，kafka，hdfs，rocksdb等等。

- Parser与Interpreter模块

  - Parser分析器负责创建AST对象
  - Interpreter解释器则负责解释AST，并进一步创建查询的执行管道（以线程的形式），返回IBlock对象。

- Functions与Aggregate Functions模块

  - 普通函数
    - 无状态，作用每一行。向量化执行。
  - 聚合函数
    - 有状态。聚合函数的状态支持序列化与反序列化，能够在分布式节点之间进行传输，以实现增量计算

- Server模块

  - 连接协议层。http，tcp，grpc，mysql，pg，prometheus

- Cluster与Replication模块

  - ClickHouse的集群由分片（Shard）组成（逻辑概念），而每个分片又通过副本（Replica）组成（物理）。

  - 1个节点只能拥有1个分片。1分片、1副本，对应实际1个物理副本（ES概念，则是2个分片，副本是特殊的分片）。


极度关注底层实现性能，通过各种数据结构和算法优化等，代码级优化（Aggregator 针对不同的数据类型使用不同的 Hash 表进行优化）。



### 10.3 优缺点

优点：

- 列存
- 数据压缩
- 向量化执行，SIMD，编译执行
- 多核并行处理
- 支持分布式查询处理，多节点
- 较完备的SQL
- 实时数据更新MergeTree
- 索引，主键索引（稀疏索引），二级索引
- 自适选择应连接算法



性能指标：

- 单个大查询的吞吐量：
  - page cache命中：2-10GB／s 处理
  - 否则，取决于磁盘系统和数据的压缩率，磁盘400MB／s的速度读取数据，数据压缩率是3，则数据的处理速度为1.2GB/s
- 处理短查询的延迟时间
  - page cache命中： < 50ms（10ms）
  - 未命中： 查找时间（10 ms） * 查询的列的数量 * 查询的数据块的数量
- 处理大量短查询的吞吐量
  - 单服务器，建议100 qps
- 数据的写入性能
  - 建议，每批1000行以上，1 qps 
  - 单insert，写入速度大约为50到200MB/s，可以多insert 并行，线性提高。

缺点：

- 无完整事务支持
- 仅批量删除或修改数据，无法高频，低延迟修改，删除单行数据。
- 不适合点查询，由于使用的是稀疏索引。
- 运维，不太方便，用户手动分片，**扩容/缩容后数据无法自动平衡，只能通过低效的数据重新导入的方式来进行人工平衡**
- 单表查询性能高，但是 Join 性能不高
  - 分布式 Join 处理方式不进行 Shuffle exchange， 不适合数据量大
  - 子查询的join，导致单点join。



### REF

- ClickHouse原理解析与应用实战-朱凯
- [clickhouse 官方中文文档](https://clickhouse.tech/docs/zh/) 可下载
- [ClickHouse 在有赞的实践之路](https://tech.youzan.com/clickhouse-zai-you-zan-de-shi-jian-zhi-lu/)





## 11.Kafka

### 11.1 场景

LinkedIn开发的分布式消息（中间件）系统，后独立成立Confluent公司，现刚上市(2021.6.25)。

kafka最新版本2.8.0。

> 起源：
>
> 开始时是用作活动流（Activity Stream）和运营数据处理管道（Pipeline），现在被广泛用于多种类型的数据管道和消息系统使用。
>
> Linked的问题——《权威指南》1.5.1节

现在定位为分布式事件流平台，用于**高性能数据管道**、**流分析**、**数据集成**和关键任务应用程序。

> 活动数据：
>
> 站点在对其网站使用情况，包括页面访问量（Page View）、被查看内容方面的信息以及搜索情况等
>
> 运营数据：
>
> 服务器的性能数据（CPU、IO 使用率、请求时间、服务日志等等数据)
>
> 传递消息：
>
> 应用程序向用户发送通知，邮件。（原神邮箱）
>
> 应用成组件只负责生成消息，不关心消息个事，何时发送。
>
> 一个公共应用程序，负责格式化消息，多个消息，同一个通知发送，根据用户配置的首选项发送数据。
>
> 日志存储：
>
> 把数据库的更新发布到Kafka上，应用程序的产生的事件流对数据库进行实时的更新。变更日志流，把对数据的更新操作，复制到远程系统，或者合并多个更新，统一更新数据库。kafka为变更日志提供缓存区，在消费者应用故障，重放日志，恢复系统状态。





#### 设计目标

- 高吞吐率。用于处理庞大的消息事件。即使在非常廉价的商用机器上也能做到单机支持每秒 100K 条以上消息的传输。
- 高性能。以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间复杂度的访问性能。处理效率不随数据量增加而降低。
- Scale out：支持在线水平扩展。
- 高可用，数据不丢。数据存储。kafka集群的服务器可跨数据中心。运行部分机器故障的存在。
- 消息传输。解耦生产者和消费者，通过持久化，允许支持多个消费者。发布与订阅能力。
- 支持离线数据处理（批量消息处理）和实时数据处理（实时流处理）。



### 11.2 基本架构

![](大数据系统鉴赏/KmCudlf7DXiAVXBMAAFScKNS-Og538.png)

Kafka组件由client端的API和部署在服务器上运行的Broker集群构成。与一般的大数据系统不一样，作为消息中间件系统，Client是一个很重要的组件。

Client API:

- Producer API：发布消息到1个或多个topic（主题）中
- Consumer API：来订阅一个或多个topic，并处理产生的消息
-  Streams API ：充当一个流处理器，从1个或多个topic消费输入流，并生产一个输出流到1个或多个输出topic，有效地将输入流转换到输出流。
- Connector API ：可构建或运行可重用的生产者或消费者，将topic连接到现有的应用程序或数据系统。例如，连接到关系数据库的连接器可以捕获表的每个变更。

Client与服务器集群的通信，使用TCP协议。

2.8版本kraft模式（旧版本还有一个Zookeeper组件，管理集群配置，选举leader。新版本已经去除，无需依赖zk，而是自己管理，更加轻量级）（服务集群只有一种broker角色，很有p2p的精神:D）



#### 基础概念

- kafka作为一个集群运行在一个或多个服务器上。

- kafka集群存储的消息是以topic为类别记录的。
  - Topic：Kafka将消息分门别类，每一类的消息称之为一个主题（Topic）

- kafka的消息结构
  - key
  - value
  - timestamp
- Broker代理
  - 消息存储器

##### 事件流

> 从技术上讲，事件流是从数据库、传感器、移动设备、云服务和软件应用程序等事件源以事件流的形式实时捕获数据的做法；
>
> 持久地存储这些事件流以供以后检索；
>
> 实时和回顾性地操作、处理和响应事件流；
>
> 并根据需要将事件流路由到不同的目标技术。

事件流应用

> - 实时处理支付和金融交易，例如在证券交易所、银行和保险中。
> - 实时跟踪和监控汽车、卡车、车队和货物，例如物流和汽车行业。
> - 持续捕获和分析来自物联网设备或其他设备（例如工厂和风电场）的传感器数据。
> - 收集客户互动和订单并立即做出反应，例如在零售、酒店和旅游行业以及移动应用程序中。
> - 监测住院病人并预测病情变化，以确保在紧急情况下得到及时治疗。
> - 连接、存储和提供公司不同部门产生的数据。
> - 作为数据平台、事件驱动架构和微服务的基础。



生产者向kafka写入事件。

消费者订阅（读取和处理）这些事件。生产者和消费者完全解耦，互不可知。

事件被组织并持久地存储在**主题**中。主题类似于文件系统中的文件夹，事件就是该文件夹中的文件。

Kafka 中的主题总是多生产者和多订阅者：一个主题可以有零个、一个或多个向其写入事件的生产者，以及零个、一个或多个订阅这些事件的消费者。

与传统消息传递系统不同，事件在消费后不会被删除。可以配置过期时间。

![](大数据系统鉴赏/streams-and-tables-p1_p4.png)

主题是**分区的**，一个主题分布在位于不同 Kafka  broker的多个“桶”上。分区的好处，在于**允许客户端应用程序同时从/向多个broker读取和写入数据**。具有相同事件key的时间被写入同一个分区。

Kafka[保证](http://kafka.apache.org/documentation/#intro_guarantees)给定主题分区的任何消费者将始终以与写入事件完全相同的顺序读取该分区的事件。

为了数据的容错性和高可用，每个主题都可以**复制**（分区级别），甚至可以跨地理区域或数据中心**复制**，以便始终有多个代理拥有数据副本。





### 11.3 优缺点

优点：

kafka作为消息系统。

天然具有解耦系统，数据的生产和使用；持久化冗余数据、日志，保证数据安全性；

扩展性，不单自己可扩展，借助消息队列，外部队列也可以根据需要调整。

削峰，应对突然的流量，避免关键组件崩溃。

异步通信；缓冲，合并消息处理；

独特优点：

顺序性的保证：分区内，消息有序；容错，允许少量机器宕机。



缺点：

kafka的高性能，一大原因是利用pagecache。

但是在单机topic和partition数量过多时，不同partition竞争pagecache。

导致整个 Broker 的处理延迟上升、吞吐下降。



挑战者：

Pulsar



### REF

- [Kafka官网](http://kafka.apache.org/)
- [Kafka 设计解析（一）：Kafka 背景及架构介绍](https://www.infoq.cn/news/kafka-analysis-part-1)
- Kafka权威指南
- Kafka stream实战-(英)
- Kafka 技术内幕-郑奇煌
- Kafka源码解析与实战-王亮
- [Kafka教程-OrcHome](https://www.orchome.com/5) 中文版翻译，持续保持更新



## 12.Flink

### 12.1 场景

Flink前身柏林理工大学的一个研究性项目，目标是要让大数据的处理看起来更加地简洁，2014年被Apache孵化器接受，是Apache社区最活跃的大数据项目。一个框架和**分布式处理引擎**（专为流处理的spark），用于对***无界和有界***数据流进行**有状态计算**。Flink 被设计为在*所有常见的集群环境中*运行，*以**内存速度***和***任何规模***执行计算

解决的问题：

- 事件时间处理语义
- 状态的一次性处理一致性保证

场景：

- 事件驱动的应用
  - 监听事件，触发异步任务完成
  - 欺诈检测，异常检测，基于规则的报警，业务流程监控
- 数据分析
  - 实时分析，低延迟；支持故障恢复，避免复杂组件的运维，故障处理逻辑
  - 电信网络质量监控，移动应用产品更新与实验评估分析，实时推荐
  - 双11成交额实时汇总，包括PV、UV的统计
- 数据管道应用
  - 有非常丰富的Connector，支持多种数据源和数据Sink，囊括了所有主流的存储系统
  - 比ETL，延迟更低，满足**实时数仓**、数据湖的需求
  - 电子商务中的实时搜索索引构建，电子商务中的持续 ETL



### 12.2 基本架构与原理

#### 架构

![](大数据系统鉴赏/flink-home-graphic.png)

#### Flink编程

![](大数据系统鉴赏/program_dataflow.svg)



分布式执行

![](大数据系统鉴赏/distributed-runtime.svg)



#### 流批一体的处理架构

![](大数据系统鉴赏/438036498d274c1dabd10650ea627c2c.png)

- **SDK 层**。Flink 的 SDK 主要有两类，第一类是关系型 Relational SDK 也就是 SQL/Table，第二类是物理型 Physical SDK 也就是 DataStream。这两类 SDK 都是流批统一，即不管是 SQL 还是 DataStream，用户的业务逻辑只要开发一遍，就可以同时在流和批的两种场景下使用；
- **执行引擎层**。执行引擎提供了统一的 DAG，用来描述数据处理流程 Data Processing Pipeline(Logical Plan)。不管是流任务还是批任务，用户的业务逻辑在执行前，都会先转化为此 DAG 图。执行引擎通过 Unified DAG Scheduler 把这个逻辑 DAG 转化成在分布式环境下执行的Task。Task 之间通过 Shuffle 传输数据，我们通过 Pluggable Unified Shuffle 架构，同时支持流批两种 Shuffle 方式；
- **状态存储**。状态存储层负责存储算子的状态执行状态。针对流作业有开源 RocksdbStatebackend、MemoryStatebackend，也有商业化的版本的GemniStateBackend；针对批作业我们在社区版本引入了 BatchStateBackend。



#### 利用内存性能

任务的状态始终保留在内存中，超过存储在磁盘。

定期和异步地对本地状态进行持久化存储来保证故障场景下精确一次的状态一致性。

![](大数据系统鉴赏/local-state.png)



### 12.3 优缺点

优点：

- 可扩展，支持上千节点，吞吐量更高，延迟更低，准确 （相比storm）
- 高性能，内存计算
- 支持SQL，有状态计算
- 高可用，无单点故障
- 容错，“Exactly-Once”处理
- 易部署，可以被yarn，k8s调度或独立集群运行
- 事件级粒度的处理，实时（相比spark  streaming  mini-bacth）
- 流批一体处理（有界，无界数据流）





### REF

- [Flink官网-中文介绍](https://flink.apache.org/zh/flink-architecture.html)
- [阿里云-Flink 必知必会经典课程1：走进 Apache Flink](https://developer.aliyun.com/article/782689) 推荐，是一个系列
- [Flink 必知必会经典课程2：Stream Processing with Apache Flink](https://developer.aliyun.com/article/782945?spm=a2c6h.12873581.0.0.6f6d2634okYVLn&groupCode=sc)
- [Flink 执行引擎：流批一体的融合之路](https://developer.aliyun.com/article/783112?spm=a2c6h.12873581.0.0.6f6d2634okYVLn&groupCode=sc)
- [七大经典技术场景！Apache Flink 在多维领域应用的 40+ 实践案例](https://developer.aliyun.com/article/783020?spm=a2c6h.12873581.0.0.6f6d2634okYVLn&groupCode=sc)
- Flink原理与实践/鲁蔚征
- Flink内核原理与实现/冯飞，崔鹏云



## 13.Pulsar



## 14.Kylin



